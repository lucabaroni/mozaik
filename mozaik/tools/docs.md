# Introduction

This document provides a guide on the format of HDF5 data files generated by the `export_from_datastore_to_hdf5` and `merge_hdf5_files` functions in `mozaik.tools.export_to_hdf5`, how to access the data within these files, and an explanation of the key functions used in the process. Additionally, it includes a usage guide for generating and accessing these files.

# Table of Contents
   1. [Introduction](#introduction)
   2. [HDF5 Data File Description](#hdf5-data-file-description)
   3. [Function Descriptions](#function-descriptions)
      - [`export_from_datastore_to_hdf5`](#export_from_datastore_to_hdf5)
      - [`merge_hdf5_files`](#merge_hdf5_files)
   4. [Usage Guide](#usage-guide)
      - [Generating an HDF5 File from a Single Dataset](#generating-an-hdf5-file-from-a-single-dataset)
      - [Generating a Merged HDF5 File](#generating-a-merged-hdf5-file)
      - [Accessing Data](#accessing-data)
        - [Example: How to Access Parameters and Datasets](#example-how-to-access-parameters-and-datasets)
        - [Example: How to Load a Specific Pair Stimulus-Response](#example-how-to-load-a-specific-pair-stimulus-response)
      - [PyTorch DataLoader Example](#pytorch-dataloader-example)


# HDF5 Data File Description

The HDF5 files generated by the `export_from_datastore_to_hdf5` function are structured to store simulation data from Mozaik in a hierarchical format. The structure is designed to efficiently store and retrieve large datasets, with metadata providing context for the data.

## Structure

- **Root Attributes**: 
  - `default_parameters`: A string representation of the default parameters used in the simulation.
  - `info`: A string representation of additional information about the simulation.
  - `data_type`: The type of data stored (e.g., 'mean_rates', 'spiketrains').
  - `st_name`: The name of the stimulus.
  - `sheets`: A list of sheets (layers) in the simulation.

- **Model Subgroups**: Each model subgroup represents a set of parameters that define a specific simulation configuration.
  - **Attributes**:
    - `parameters`: A string representation of the parameters for the model.

- **Stimulus Subgroups**: Each stimulus subgroup contains data related to a specific stimulus.
  - **Attributes**:
    - `Varying parameters`: A list of parameters that vary across stimuli.
    - `Constant parameters`: A list of parameters that remain constant across stimuli.
    - `Data dimensions`: Dimensions of the data based on varying parameters.
    - `data_type`, `data_cut_start`, `data_cut_end`: Metadata about the data type and extraction time window.

- **Datasets**: Each sheet within a stimulus subgroup contains datasets corresponding to the data type (e.g., mean rates, spiketrains).

# Function Descriptions

## `export_from_datastore_to_hdf5`
The `export_from_datastore_to_hdf5` function is designed to export data from a Mozaik datastore into an HDF5 file, providing a structured and efficient way to store and retrieve large datasets. All essential information is stored as metadata within the HDF5 file.

#### Parameters

- **`data_store`**: The datastore containing simulation data.
- **`st_name`**: The name of the stimulus to be exported.
- **`data_type`**: The type of data to be exported, such as 'mean_rates', 'spiketrains', or 'segments'.
- **`cut_start`** and **`cut_end`**: Optional parameters that define the time window for data extraction.

#### Process Overview

The exporting process is composed of multiple steps. Here we provide a high-level overview:

1. **Model Information Retrieval**:
   - The process begins by setting up the necessary paths and loading model parameters from the base folder specified in the datastore. This is achieved using the `get_model_info_and_parameters` function. Differences between modified and default parameters are identified, and PyNN distribution parameters are serialized using the `serialize_parameters` function to ensure they can be stored as metadata.

2. **Parameter Classification**:
   - The function analyzes the list of stimuli to classify their parameters into constant and varying categories using the `classify_stimulus_parameters_into_constant_and_varying` function. Constant parameters remain unchanged across all stimuli, while varying parameters differ and are sorted to facilitate data organization.

3. **HDF5 Structure Creation**:
   - A new HDF5 file is initialized in the base folder. Metadata such as default parameters, simulation info, sheet names, stimulus name, and data type are added as attributes to the root of the file. For each model configuration, a subgroup is created using the `create_hdf5_structure` function. If no parameters are modified, this subgroup is named 'default'; otherwise, it is named based on the modified parameters. Within each model subgroup, a stimuli subgroup is created for the specified stimulus, storing both varying and constant parameters as attributes.

4. **Data Extraction and Storage**:
   - The function iterates over each sheet in the datastore to extract relevant data segments using the `extract_sheet_data_and_save_to_h5py` function. Depending on the specified data type and optional time window, the data is reshaped and reordered according to the varying parameters. This reshaped data is then stored in the appropriate subgroup within the HDF5 file.

 5. **Stimuli Dataset Addition**: Finally, the function adds two datasets: `stimuli` and `stimuli_idx`. The `stimuli` dataset lists all stimuli, providing essential context for interpreting the response data. The `stimuli_idx` dataset mirrors the shape of the sheet-specific response datasets (up to the last dimension) and is used to accurately link each stimulus to its corresponding response data.

## `merge_hdf5_files`

This function merges multiple HDF5 files into a single file, assuming they were generated from similar Mozaik simulations with varying parameters. It ensures that the merged file maintains the original hierarchical structure and data integrity.

##### Parameters

- **`file_list`**: A list of paths to the input HDF5 files.
- **`output_file`**: The path to the output merged HDF5 file.

#### Process

1. **Open Files**: 
   - Opens all input HDF5 files and initializes variables to store common information such as default parameters, simulation info, and sheet names.
   - Ensures that all files have consistent metadata, including default parameters, info, sheets, data types, and stimulus names. This consistency is crucial for a successful merge, as discrepancies in these attributes will prevent merging.

2. **Merge Data**: 
   - Identify the varying parameters that differ across the input files. This parameter is used to determine how the data should be merged.
   - Combines data along the dimensions that vary across files, ensuring that all possible combinations of varying parameters are represented in the merged file.

3. **Write Merged Data**: 
   - Writes the merged data to the output HDF5 file, maintaining the original hierarchical structure. This includes creating groups and subgroups for models and stimuli, and adding datasets for each sheet.
   - Additional metadata is added to the merged file, including information about the origins of the data and the varying parameters that were used in the merge.

#### Conditions for Merging

- All input files must share identical default parameters, simulation info, sheet names, data types, and stimulus names.
- Exactly one varying parameter should have distinct, non-overlapping values across files to enable meaningful data aggregation. If no such parameter exists, or if multiple parameters differ, or if values overlap, the merge is skipped.


# Usage Guide

## Generating an HDF5 File from a Single Dataset

To generate an HDF5 file from a single dataset, use the `export_from_datastore_to_hdf5` function. This function can be run just after the simulation has been run, and will organize the data into a hierarchical structure within the HDF5 file. 

## Generating a Merged HDF5 File

To generate a merged HDF5 file from multiple files, use the `merge_hdf5_files` function. This is particularly useful for aggregating data from simulations with varying parameters, such as those generated in a Mozaik parameter search.

## Accessing Data

To access data stored in the HDF5 files, you can use the `h5py` library in Python. Below is a minimal example demonstrating how to read data from an HDF5 file.

### Example: How to Access Parameters and Datasets

```python
import h5py

# Open the HDF5 file
with h5py.File('path/to/your/hdf5_file.h5', 'r') as f:
    # Access root attributes
    default_parameters = eval(f.attrs['default_parameters'])
    info = eval(f.attrs['info'])
    sheets = f.attrs['sheets']

    # Iterate over model subgroups
    for model_key in f.keys():
        model_subgroup = f[model_key]
        model_parameters = eval(model_subgroup.attrs['parameters'])

        # Iterate over stimulus subgroups
        for stim_key in model_subgroup.keys():
            stim_subgroup = model_subgroup[stim_key]
            varying_params = stim_subgroup.attrs['Varying parameters']
            constant_params = stim_subgroup.attrs['Constant parameters']

            # Access datasets
            for sheet in sheets:
                data = stim_subgroup[sheet][:]
                print(f"Data for sheet {sheet}: {data}")

            # Access stimuli and indices
            stimuli = stim_subgroup['stimuli'][:]
            stimuli_idx = stim_subgroup['stimuli_idx'][:]
            print(f"Stimuli: {stimuli}")
            print(f"Stimuli Indices: {stimuli_idx}")
```

### Example: How to Load a Specific Pair Stimulus-Response

```python
import h5py
import numpy as np
import matplotlib.pyplot as plt

def get_stimulus_response_pairs(file_path, model_key, stim_key, sheet, indices):
    """
    Access specific pairs of stimuli and responses in the HDF5 file.
    
    Parameters:
    file_path (str): Path to the HDF5 file
    model_key (str): Key for the model subgroup
    stim_key (str): Key for the stimulus subgroup
    sheet (str): Name of the sheet (e.g., 'V1_Exc_L23')
    indices (tuple, array or list of int): Indices of the stimuli to retrieve. Length of indices must match the number of dimensions of the stimuli index dataset.
    
    Returns:
    tuple: (stimuli, responses)
    """
    with h5py.File(file_path, 'r') as f:
        # Convert indices to tuple
        indices = tuple(indices)

        # Navigate to the specific subgroup
        subgroup = f[model_key][stim_key]
        
        # Get the stimuli
        stimuli_idx_dataset = subgroup['stimuli_idx'][:]
        stimuli_dataset = subgroup['stimuli'][:]
      
        stimulus_idx = stimuli_idx_dataset[indices]
        stimuli = stimuli_dataset[stimulus_idx]
        
        # Get the responses
        response_dataset = subgroup[sheet]
        responses = response_dataset[indices]
        return stimuli, responses

# Example usage:
file_path = 'path/to/your/hdf5/file.h5'  # Replace with your actual file path
model_key = 'default'
stim_key = 'NaturalImage'
sheet = 'V1_Exc_L23'
indices = np.array([1, 3])  # Example list of indices

stimuli, responses = get_stimulus_response_pairs(file_path, model_key, stim_key, sheet, indices)
print(stimuli.shape)
print(responses.shape)
```  

### PyTorch DataLoader Example
TO BE WRITTEN

<!--
To efficiently load data from HDF5 files into PyTorch, you can create a custom `Dataset` class and use a `DataLoader`. Below is an example of how to access stimuli and responses from an HDF5 file using PyTorch.

```python
import h5py
import torch
from torch.utils.data import Dataset, DataLoader

class HDF5Dataset(Dataset):
    def __init__(self, file_path):
        self.file = h5py.File(file_path, 'r')
        self.stimuli = self.file['stimuli'][:]
        self.responses = self.file['responses'][:]

    def __len__(self):
        return len(self.stimuli)

    def __getitem__(self, idx):
        stimulus = self.stimuli[idx]
        response = self.responses[idx]
        return torch.tensor(stimulus), torch.tensor(response)

# Usage
dataset = HDF5Dataset('path/to/your/hdf5_file.h5')
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

for batch in dataloader:
    stimuli, responses = batch
    # Process your batch
```

This example demonstrates how to create a PyTorch `Dataset` that reads from an HDF5 file, allowing you to use PyTorch's `DataLoader` for efficient data loading and batching.
-->
